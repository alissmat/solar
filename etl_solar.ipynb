{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üôã‚Äç‚ôÇÔ∏è Welcome to the TMY weekly solar data aggregator ‚òÄÔ∏è\n",
    "\n",
    "This notebook runs an ETL procedure on solar data from various locations in the USA.\n",
    "\n",
    "### Data source\n",
    "__Source link:__ [tmy3-solar](https://www.kaggle.com/datasets/us-doe/tmy3-solar/code)\n",
    "\n",
    "__File:__ `tmy3.csv`  \n",
    "* __Description:__ One Year of typical Hourly Solar & Weather Data for +1000 US Locations\n",
    "* __Data points:__ \n",
    "    * Date (MM/DD/YYYY)\n",
    "    * Time (HH:MM)\n",
    "    * GHI (W/m^2)\n",
    "    * DNI (W/m^2)\n",
    "    * station_number\n",
    "\n",
    "__File:__ `TMY3_StationsMeta.csv`  \n",
    "* __Description:__ Metadata about weather stations\n",
    "* __Data points:__ \n",
    "    * USAF\n",
    "    * Site Name\n",
    "    * Latitude\n",
    "    * Longitude\n",
    "\n",
    "__File:__ `43256.pdf`\n",
    "* __Description:__ User Guide for TMY3 data\n",
    "\n",
    "\n",
    "### Transformations\n",
    "This code aggregates the hourly GHI and DNI values in `tmy3.csv` into weekly averages for each station.\n",
    "\n",
    "### Outputs\n",
    "A .json file at a specified path.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Set up\n",
    "Ensure you have a Kaggle API access key at: `C:\\Users\\<your_name>\\.kaggle\\kaggle.json`  \n",
    "\n",
    "Check the README for further info on installing required packages if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to create a conda environment 'solar' with the required packages\n",
    "# !conda env create -f environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import opendatasets as od \n",
    "import json\n",
    "from datetime import timedelta\n",
    "\n",
    "# specify the output filepath\n",
    "output_path = 'output.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚¨áÔ∏è Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_data = \"https://www.kaggle.com/datasets/us-doe/tmy3-solar\"\n",
    "od.download_kaggle_dataset(dataset_url=kaggle_data, data_dir=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv into a dataframe\n",
    "tmy3 = pd.read_csv(r\"data\\tmy3-solar\\tmy3.csv\", usecols=['Date (MM/DD/YYYY)', 'Time (HH:MM)', 'GHI (W/m^2)', 'DNI (W/m^2)', 'station_number'])\n",
    "tmy3.columns = ['date', 'time', 'ghi', 'dni', 'station']\n",
    "tmy3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metadata\n",
    "tmy3_meta = pd.read_csv(r\"data\\tmy3-solar\\TMY3_StationsMeta.csv\")\n",
    "tmy3_meta.set_index('USAF', inplace=True)\n",
    "tmy3_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleaning\n",
    "\n",
    "### __NaN__\n",
    "There are a few NaN values in the dataset.  These should be dropped, as they represent missing data and may distort aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are a few NA values.  These should be dropped as they represent missing data.\n",
    "tmy3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmy3.dropna(inplace=True)\n",
    "tmy3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __24:00__\n",
    "The source csv file includes 24-hour data with the value '24:00' indicating midnight. The ISO 8601 standard uses 00:00 for midnight and does not use 24:00.  Therefore, rows in the dataset with a time value of 24:00 will need to be changed to 00:00.  The date of these rows will also need to increment by 1. \n",
    "\n",
    "__Example__  \n",
    "Incorrect: 2000-01-01T24:00:00  \n",
    "Correct: 20000-01-02T00:00:00\n",
    "\n",
    "Therefore this cleaning step will replace some time values and increment the date of those values.\n",
    "\n",
    "This step also creates a 'timestamp' column with pandas datetime objects to use as an index.  This will assist with date operations and aggregating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify items with time = 24:00 and change to 00:00\n",
    "mask_24 = tmy3['time'] == '24:00'\n",
    "tmy3.loc[mask_24, 'time'] = '00:00'\n",
    "\n",
    "# create 'timestamp' column with pandas datetime objects\n",
    "tmy3['timestamp'] = pd.to_datetime(tmy3['date'] + ' ' + tmy3['time'], format='%m/%d/%Y %H:%M')\n",
    "\n",
    "# increment the 'timestamp' column by one for items that were changed from 24:00 to 00:00\n",
    "tmy3.loc[mask_24, 'timestamp'] += timedelta(days=1)\n",
    "\n",
    "# drop the 'date' and 'time' columns, as they are no longer needed.  The 'date' column is also incorrect now.\n",
    "tmy3.drop(['date', 'time'], axis=1, inplace=True)\n",
    "\n",
    "# set the index as the 'timestamp' column, which is required for further processing steps\n",
    "tmy3.set_index('timestamp', inplace=True)\n",
    "\n",
    "# inspect the data\n",
    "tmy3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü¶© Wrangling\n",
    "\n",
    "__NB:__ \n",
    "When you use `pandas.resample('W')` to resample data by week, the following rules apply:\n",
    "\n",
    "1. Included Timestamps in Each Week  \n",
    "\n",
    "    __Weekly Grouping:__ `resample('W')` groups data from Monday through Sunday. Each group includes all timestamps from the beginning of Monday at 00:00:00 to the end of Sunday at 23:59:59.\n",
    "\n",
    "    __Resulting Timestamp__: The resulting timestamp for each week is set to the end of that week, which is Sunday.\n",
    "\n",
    "    __Non-Sunday Starting Weeks:__ If your data does not start on a Monday, the first \"partial week\" will still include data from the first available day up to that Saturday.\n",
    "\n",
    "2. Customizing the Week Ending Day  \n",
    "\n",
    "    You can change the default behavior of grouping weeks from Sunday to Saturday by using `resample('W-MON')`, `resample('W-FRI')`, etc., where the argument specifies which day of the week the resampling should end on:\n",
    "\n",
    "    `resample('W-MON')`: Groups from Tuesday to Monday, with the resulting timestamp set to Monday.  \n",
    "    `resample('W-FRI')`: Groups from Saturday to Friday, with the resulting timestamp set to Friday.  \n",
    "\n",
    "_(source: ChatGPT)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 'ghi'\n",
    "station = 690150\n",
    "week_end = pd.to_datetime('1996-08-11 23:00')\n",
    "week_start = week_end - timedelta(days=7)\n",
    "\n",
    "# check a value in the existing df to validate the transform\n",
    "df = tmy3.query(\"station == @station and @week_start < timestamp <= @week_end \")\n",
    "print(f\"Mean '{value}' at station {station} for week ending {week_end} is: \\n{df[value].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Perform transformations on the Dataframe object. This function aggregates by station, and then\n",
    "    resamples the 'ghi' and 'dni' values to a weekly mean.\n",
    "    \"\"\"\n",
    "    output = df.groupby('station').resample('W').mean(['ghi', 'dni']).drop('station', axis=1).fillna(0)\n",
    "    print('Data has been transformed.')\n",
    "    return output\n",
    "\n",
    "tmy3_transform = wrangle(tmy3)\n",
    "tmy3_transform.query(\"station == @station\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚¨ÜÔ∏è Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output = [] \n",
    "\n",
    "for station, group in tmy3_transform.groupby('station'):\n",
    "    # get site information from the metadata df\n",
    "    site_name = tmy3_meta.loc[station, 'Site Name']\n",
    "    coordinates = [tmy3_meta.loc[station, 'Latitude'], tmy3_meta.loc[station, 'Longitude']]\n",
    "    \n",
    "    # compile the data points for each station\n",
    "    data_list = []\n",
    "    for timestamp, row in group.iterrows():\n",
    "        data_entry = {\n",
    "            # get the timestamp in milliseconds since epoch\n",
    "            'timestamp': int(timestamp[1].timestamp() * 1000), \n",
    "            'ghi': row['ghi'],\n",
    "            'dni': row['dni']\n",
    "        }\n",
    "        data_list.append(data_entry)\n",
    "\n",
    "    # compile the full json for each station\n",
    "    json_entry = {\n",
    "        'id': station,\n",
    "        'site_name': site_name,\n",
    "        'coordinates': coordinates, \n",
    "        'data': data_list\n",
    "    }\n",
    "\n",
    "    json_output.append(json_entry)\n",
    "\n",
    "# output to file\n",
    "with open(output_path, 'w') as file:\n",
    "    json.dump(json_output, file, indent=4)\n",
    "\n",
    "print(f'Data has been saved to {output_path}')\n",
    "\n",
    "# inspect the output\n",
    "print(json.dumps(json_output, indent=4, default=str))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
